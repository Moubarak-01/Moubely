import { AppState } from "./main"
import { LLMHelper } from "./LLMHelper"
import dotenv from "dotenv"

dotenv.config()

export class ProcessingHelper {
  private appState: AppState
  private llmHelper: LLMHelper
  private currentProcessingAbortController: AbortController | null = null
  private currentExtraProcessingAbortController: AbortController | null = null

  constructor(appState: AppState) {
    this.appState = appState
    const useOllama = process.env.USE_OLLAMA === "true"
    if (useOllama) {
      console.log("[Processing] üîß Initializing with Ollama");
      this.llmHelper = new LLMHelper(undefined, true, process.env.OLLAMA_MODEL, process.env.OLLAMA_URL)
    } else {
      console.log("[Processing] ‚òÅÔ∏è Initializing with Cloud APIs");
      const apiKey = process.env.GEMINI_API_KEY
      if (!apiKey) throw new Error("GEMINI_API_KEY missing")
      this.llmHelper = new LLMHelper(apiKey, false)
    }
  }

  public async processScreenshots(): Promise<void> {
    console.log("[Processing] üöÄ Processing Screenshots triggered");
    
    const mainWindow = this.appState.getMainWindow()
    if (!mainWindow) return
    const view = this.appState.getView()

    if (view === "queue") {
      const screenshotQueue = this.appState.getScreenshotHelper().getScreenshotQueue()
      if (screenshotQueue.length === 0) {
        console.log("[Processing] ‚ö†Ô∏è Screenshot queue is empty");
        mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.NO_SCREENSHOTS)
        return
      }
      const lastPath = screenshotQueue[screenshotQueue.length - 1];
      
      console.log(`[Processing] üîç Analyzing last screenshot: ${lastPath}`);
      mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.INITIAL_START)
      this.appState.setView("solutions")
      this.currentProcessingAbortController = new AbortController()
      
      try {
        if (lastPath.endsWith('.mp3') || lastPath.endsWith('.wav')) {
             console.log("[Processing] üéôÔ∏è Detected Audio File - Starting Transcription");
             const audioResult = await this.llmHelper.analyzeAudioFile(lastPath);
             const problem = { problem_statement: audioResult.text, input_format: {}, output_format: {}, constraints: [], test_cases: [] };
             console.log("[Processing] ‚úÖ Audio Processed Successfully");
             
             mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.PROBLEM_EXTRACTED, problem);
             this.appState.setProblemInfo(problem);
        } else {
             console.log("[Processing] üñºÔ∏è Detected Image File - Starting Visual Analysis");
             const imageResult = await this.llmHelper.analyzeImageFile(lastPath);
             const problem = { problem_statement: imageResult.text, input_format: {}, output_format: {}, constraints: [], test_cases: [] };
             console.log("[Processing] ‚úÖ Image Processed Successfully");
             
             mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.PROBLEM_EXTRACTED, problem);
             this.appState.setProblemInfo(problem);
        }
      } catch (error: any) {
        console.error(`[Processing] ‚ùå Error processing file: ${error.message}`);
        mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.INITIAL_SOLUTION_ERROR, error.message)
      } finally {
        this.currentProcessingAbortController = null
      }
    } else {
      // --- DEBUGGING / EXTRA SCREENSHOTS MODE ---
      const extraScreenshotQueue = this.appState.getScreenshotHelper().getExtraScreenshotQueue()
      if (extraScreenshotQueue.length === 0) {
        console.log("[Processing] ‚ö†Ô∏è Extra screenshot queue is empty");
        mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.NO_SCREENSHOTS)
        return
      }
      
      console.log(`[Processing] üêû Starting Debug Flow with ${extraScreenshotQueue.length} new screenshots`);
      mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.DEBUG_START)
      this.currentExtraProcessingAbortController = new AbortController()

      try {
        const problemInfo = this.appState.getProblemInfo()
        if (!problemInfo) throw new Error("No problem info available for debugging")

        console.log("[Processing] üß† Generating current solution context...");
        // üëá FIX: Cast to 'any' to fix TS build error
        const currentSolution = await this.llmHelper.generateSolution(problemInfo) as any
        const currentCode = currentSolution?.solution?.code || "No code"

        console.log("[Processing] üïµÔ∏è analyzing new screenshots against current code...");
        const debugResult = await this.llmHelper.debugSolutionWithImages(problemInfo, currentCode, extraScreenshotQueue)
        
        console.log("[Processing] ‚úÖ Debugging/Refinement Successful");
        this.appState.setHasDebugged(true)
        mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.DEBUG_SUCCESS, debugResult)
      } catch (error: any) {
        console.error(`[Processing] ‚ùå Debugging Failed: ${error.message}`);
        mainWindow.webContents.send(this.appState.PROCESSING_EVENTS.DEBUG_ERROR, error.message)
      } finally {
        this.currentExtraProcessingAbortController = null
      }
    }
  }

  public cancelOngoingRequests() {
    console.log("[Processing] üõë Cancelling ongoing requests");
    if (this.currentProcessingAbortController) this.currentProcessingAbortController.abort()
    if (this.currentExtraProcessingAbortController) this.currentExtraProcessingAbortController.abort()
    this.appState.setHasDebugged(false)
  }
  
  public async processAudioBase64(data: string, mimeType: string) { 
      // console.log("[Processing] üéôÔ∏è Processing Audio Chunk..."); // Optional log
      return this.llmHelper.analyzeAudioFromBase64(data, mimeType); 
  }
  
  public async processAudioFile(filePath: string) { 
      console.log(`[Processing] üìÅ Process Audio File: ${filePath}`);
      return this.llmHelper.analyzeAudioFile(filePath); 
  }
  
  public getLLMHelper() { return this.llmHelper; }
}